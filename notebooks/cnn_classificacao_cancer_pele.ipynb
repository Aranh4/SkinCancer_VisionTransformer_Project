{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üî¨ Classifica√ß√£o de C√¢ncer de Pele com CNN\n",
        "\n",
        "Este notebook implementa uma **Rede Neural Convolucional (CNN)** para classificar les√µes de pele usando o dataset **HAM10000**.\n",
        "\n",
        "## üìä Objetivo\n",
        "\n",
        "Classificar imagens dermatosc√≥picas em **7 categorias** diferentes de les√µes de pele:\n",
        "1. **Melanoma (mel)** - Melanoma maligno\n",
        "2. **Nevos melanoc√≠ticos (nv)** - Nevos melanoc√≠ticos benignos\n",
        "3. **Carcinoma basocelular (bcc)** - Carcinoma basocelular\n",
        "4. **Queratose act√≠nica (akiec)** - Queratose act√≠nica / Carcinoma in situ\n",
        "5. **Queratose benigna (bkl)** - Queratose benigna\n",
        "6. **Dermatofibroma (df)** - Dermatofibroma\n",
        "7. **Les√µes vasculares (vasc)** - Les√µes vasculares\n",
        "\n",
        "## üéØ Abordagem\n",
        "\n",
        "Vamos usar **apenas as imagens** (an√°lise puramente visual) para a classifica√ß√£o, sem utilizar metadados como idade, sexo ou localiza√ß√£o. Isso permitir√° compara√ß√£o justa com modelos Vision Transformer e VLMs posteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 1Ô∏è‚É£ Setup e Importa√ß√µes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar para n√£o exibir warnings desnecess√°rios\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Bibliotecas essenciais\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "\n",
        "# Scikit-learn para m√©tricas e divis√£o de dados\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# TensorFlow e Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Configurar seeds para reprodutibilidade\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Configura√ß√µes de visualiza√ß√£o\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "print(f\"TensorFlow vers√£o: {tf.__version__}\")\n",
        "print(f\"GPU dispon√≠vel: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìå Configura√ß√µes Globais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dimens√µes das imagens (reduzidas para treino mais r√°pido)\n",
        "IMG_WIDTH = 100\n",
        "IMG_HEIGHT = 75\n",
        "IMG_CHANNELS = 3\n",
        "\n",
        "# Hiperpar√¢metros\n",
        "BATCH_SIZE = 10\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# N√∫mero de classes\n",
        "NUM_CLASSES = 7\n",
        "\n",
        "# Caminhos\n",
        "BASE_DIR = os.path.join('..', 'data', 'raw')\n",
        "MODELS_DIR = os.path.join('..', 'models')\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Diret√≥rio base: {BASE_DIR}\")\n",
        "print(f\"Diret√≥rio de modelos: {MODELS_DIR}\")\n",
        "print(f\"\\nConfigura√ß√µes:\")\n",
        "print(f\"  Imagens: {IMG_WIDTH}x{IMG_HEIGHT}x{IMG_CHANNELS}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 2Ô∏è‚É£ Carregamento e Prepara√ß√£o dos Dados\n",
        "\n",
        "Vamos carregar os metadados e criar um mapeamento para as imagens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregar metadados\n",
        "metadata_path = os.path.join(BASE_DIR, 'HAM10000_metadata.csv')\n",
        "df = pd.read_csv(metadata_path)\n",
        "\n",
        "print(f\"üìä Total de imagens: {len(df)}\")\n",
        "print(f\"\\nüîç Primeiras linhas:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar mapeamento image_id -> caminho completo da imagem\n",
        "# As imagens est√£o em duas pastas diferentes\n",
        "image_paths = {}\n",
        "for folder in ['HAM10000_images_part_1', 'HAM10000_images_part_2']:\n",
        "    folder_path = os.path.join(BASE_DIR, folder)\n",
        "    if os.path.exists(folder_path):\n",
        "        for img_file in glob(os.path.join(folder_path, '*.jpg')):\n",
        "            image_id = os.path.splitext(os.path.basename(img_file))[0]\n",
        "            image_paths[image_id] = img_file\n",
        "\n",
        "print(f\"‚úÖ Total de imagens encontradas: {len(image_paths)}\")\n",
        "\n",
        "# Adicionar caminho das imagens ao dataframe\n",
        "df['image_path'] = df['image_id'].map(image_paths)\n",
        "\n",
        "# Verificar se todas as imagens foram encontradas\n",
        "missing = df['image_path'].isna().sum()\n",
        "if missing > 0:\n",
        "    print(f\"‚ö†Ô∏è ATEN√á√ÉO: {missing} imagens n√£o foram encontradas!\")\n",
        "    df = df.dropna(subset=['image_path'])\n",
        "else:\n",
        "    print(\"‚úÖ Todas as imagens foram encontradas!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dicion√°rio com nomes amig√°veis das classes\n",
        "class_names = {\n",
        "    'nv': 'Nevos melanoc√≠ticos',\n",
        "    'mel': 'Melanoma',\n",
        "    'bkl': 'Queratose benigna',\n",
        "    'bcc': 'Carcinoma basocelular',\n",
        "    'akiec': 'Queratose act√≠nica',\n",
        "    'vasc': 'Les√µes vasculares',\n",
        "    'df': 'Dermatofibroma'\n",
        "}\n",
        "\n",
        "# Adicionar nome amig√°vel e √≠ndice num√©rico\n",
        "df['class_name'] = df['dx'].map(class_names)\n",
        "df['class_idx'] = pd.Categorical(df['dx']).codes\n",
        "\n",
        "print(\"\\nüìã Mapeamento de classes:\")\n",
        "for idx, (code, name) in enumerate(class_names.items()):\n",
        "    count = (df['dx'] == code).sum()\n",
        "    print(f\"  {idx}: {code:6s} -> {name:30s} ({count:4d} imagens)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üßπ Limpeza de Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar valores faltantes\n",
        "print(\"‚ùì Valores faltantes por coluna:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Preencher idade faltante com a m√©dia\n",
        "df['age'].fillna(df['age'].mean(), inplace=True)\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset limpo com {len(df)} registros\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 3Ô∏è‚É£ An√°lise Explorat√≥ria de Dados (EDA)\n",
        "\n",
        "Vamos entender a distribui√ß√£o dos dados antes de treinar o modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Distribui√ß√£o das Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
        "\n",
        "# Contar por classe\n",
        "class_counts = df['class_name'].value_counts()\n",
        "\n",
        "# Plotar\n",
        "class_counts.plot(kind='bar', ax=ax, color=sns.color_palette(\"husl\", len(class_counts)))\n",
        "ax.set_title('Distribui√ß√£o das Classes de Les√µes de Pele', fontsize=16, fontweight='bold')\n",
        "ax.set_xlabel('Tipo de Les√£o', fontsize=12)\n",
        "ax.set_ylabel('Quantidade de Imagens', fontsize=12)\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# Adicionar valores nas barras\n",
        "for i, v in enumerate(class_counts.values):\n",
        "    ax.text(i, v + 100, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è OBSERVA√á√ÉO IMPORTANTE:\")\n",
        "print(\"O dataset √© altamente DESBALANCEADO!\")\n",
        "print(f\"Classe majorit√°ria: {class_counts.index[0]} ({class_counts.values[0]} imagens)\")\n",
        "print(f\"Classe minorit√°ria: {class_counts.index[-1]} ({class_counts.values[-1]} imagens)\")\n",
        "print(f\"Raz√£o: {class_counts.values[0] / class_counts.values[-1]:.1f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üë• Outras Distribui√ß√µes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Distribui√ß√£o de idade\n",
        "axes[0].hist(df['age'], bins=30, color='skyblue', edgecolor='black')\n",
        "axes[0].set_title('Distribui√ß√£o de Idade', fontweight='bold')\n",
        "axes[0].set_xlabel('Idade (anos)')\n",
        "axes[0].set_ylabel('Frequ√™ncia')\n",
        "\n",
        "# Distribui√ß√£o por sexo\n",
        "df['sex'].value_counts().plot(kind='bar', ax=axes[1], color=['lightcoral', 'lightblue', 'lightgray'])\n",
        "axes[1].set_title('Distribui√ß√£o por Sexo', fontweight='bold')\n",
        "axes[1].set_xlabel('Sexo')\n",
        "axes[1].set_ylabel('Quantidade')\n",
        "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)\n",
        "\n",
        "# Distribui√ß√£o por localiza√ß√£o (top 10)\n",
        "top_locations = df['localization'].value_counts().head(10)\n",
        "top_locations.plot(kind='barh', ax=axes[2], color='lightgreen')\n",
        "axes[2].set_title('Top 10 Localiza√ß√µes', fontweight='bold')\n",
        "axes[2].set_xlabel('Quantidade')\n",
        "axes[2].set_ylabel('Localiza√ß√£o')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìù Nota: Embora interessantes, N√ÉO vamos usar esses dados no modelo.\")\n",
        "print(\"   Vamos classificar APENAS com base nas imagens!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üñºÔ∏è Visualiza√ß√£o de Amostras de Cada Classe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotar 5 exemplos de cada classe\n",
        "n_samples = 5\n",
        "fig, axes = plt.subplots(NUM_CLASSES, n_samples, figsize=(n_samples * 3, NUM_CLASSES * 3))\n",
        "\n",
        "for idx, (class_code, class_label) in enumerate(class_names.items()):\n",
        "    # Pegar amostras aleat√≥rias da classe\n",
        "    class_df = df[df['dx'] == class_code]\n",
        "    n_available = min(n_samples, len(class_df))\n",
        "    class_samples = class_df.sample(n_available, random_state=SEED)\n",
        "    \n",
        "    for col, (_, row) in enumerate(class_samples.iterrows()):\n",
        "        img = Image.open(row['image_path'])\n",
        "        axes[idx, col].imshow(img)\n",
        "        axes[idx, col].axis('off')\n",
        "        \n",
        "        # Adicionar t√≠tulo apenas na primeira coluna\n",
        "        if col == 0:\n",
        "            axes[idx, col].text(-10, img.size[1]//2, class_label, \n",
        "                              fontsize=10, fontweight='bold', \n",
        "                              rotation=90, va='center', ha='right')\n",
        "\n",
        "plt.suptitle('Amostras de Cada Classe de Les√£o', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 4Ô∏è‚É£ Carregamento e Processamento de Imagens\n",
        "\n",
        "Agora vamos carregar todas as imagens, redimension√°-las e convert√™-las em arrays numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"‚è≥ Carregando e redimensionando {len(df)} imagens para {IMG_WIDTH}x{IMG_HEIGHT}...\")\n",
        "print(\"   Isso pode levar alguns minutos...\")\n",
        "\n",
        "# Fun√ß√£o para carregar e redimensionar imagem\n",
        "def load_and_resize_image(path):\n",
        "    try:\n",
        "        img = Image.open(path)\n",
        "        img = img.resize((IMG_WIDTH, IMG_HEIGHT))\n",
        "        return np.array(img)\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar {path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Carregar imagens\n",
        "df['image_array'] = df['image_path'].apply(load_and_resize_image)\n",
        "\n",
        "# Remover imagens que falharam\n",
        "df = df.dropna(subset=['image_array'])\n",
        "\n",
        "print(f\"‚úÖ {len(df)} imagens carregadas com sucesso!\")\n",
        "print(f\"   Shape de cada imagem: {df['image_array'].iloc[0].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar distribui√ß√£o de tamanhos\n",
        "shapes = df['image_array'].apply(lambda x: x.shape).value_counts()\n",
        "print(\"\\nüìê Distribui√ß√£o de shapes:\")\n",
        "print(shapes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 5Ô∏è‚É£ Divis√£o dos Dados (Train/Validation/Test)\n",
        "\n",
        "Vamos dividir em:\n",
        "- **80%** para treinamento  \n",
        "- **10%** para valida√ß√£o (do conjunto de treino)  \n",
        "- **20%** para teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar features (X) e labels (y)\n",
        "X = np.array(df['image_array'].tolist())\n",
        "y = df['class_idx'].values\n",
        "\n",
        "print(f\"üìä Shape dos dados:\")\n",
        "print(f\"   X: {X.shape}\")\n",
        "print(f\"   y: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Primeira divis√£o: Train (80%) e Test (20%)\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.20, \n",
        "    random_state=SEED,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# Segunda divis√£o: Train (90% de 80%) e Validation (10% de 80%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full, y_train_full,\n",
        "    test_size=0.10,\n",
        "    random_state=SEED,\n",
        "    stratify=y_train_full\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Divis√£o dos dados:\")\n",
        "print(f\"   Treino:     {X_train.shape[0]:5d} imagens ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"   Valida√ß√£o:  {X_val.shape[0]:5d} imagens ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"   Teste:      {X_test.shape[0]:5d} imagens ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"   TOTAL:      {len(X):5d} imagens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 6Ô∏è‚É£ Normaliza√ß√£o e Encoding\n",
        "\n",
        "### üî¢ Normaliza√ß√£o das Imagens\n",
        "\n",
        "Vamos normalizar os pixels para terem **m√©dia 0 e desvio padr√£o 1**. Isso ajuda a rede neural a convergir mais r√°pido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular m√©dia e desvio padr√£o do conjunto de treino\n",
        "X_train_mean = np.mean(X_train)\n",
        "X_train_std = np.std(X_train)\n",
        "\n",
        "print(f\"üìä Estat√≠sticas do conjunto de treino (antes da normaliza√ß√£o):\")\n",
        "print(f\"   M√©dia: {X_train_mean:.2f}\")\n",
        "print(f\"   Desvio padr√£o: {X_train_std:.2f}\")\n",
        "print(f\"   Min: {X_train.min()}\")\n",
        "print(f\"   Max: {X_train.max()}\")\n",
        "\n",
        "# Normalizar todos os conjuntos usando estat√≠sticas do treino\n",
        "X_train = (X_train - X_train_mean) / X_train_std\n",
        "X_val = (X_val - X_train_mean) / X_train_std\n",
        "X_test = (X_test - X_train_mean) / X_train_std\n",
        "\n",
        "print(f\"\\nüìä Ap√≥s normaliza√ß√£o:\")\n",
        "print(f\"   M√©dia: {X_train.mean():.6f}\")\n",
        "print(f\"   Desvio padr√£o: {X_train.std():.6f}\")\n",
        "print(f\"\\n‚úÖ Normaliza√ß√£o conclu√≠da!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üè∑Ô∏è One-Hot Encoding dos Labels\n",
        "\n",
        "Converter labels num√©ricos (0-6) em vetores one-hot para classifica√ß√£o multiclasse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-hot encoding\n",
        "y_train_encoded = to_categorical(y_train, num_classes=NUM_CLASSES)\n",
        "y_val_encoded = to_categorical(y_val, num_classes=NUM_CLASSES)\n",
        "y_test_encoded = to_categorical(y_test, num_classes=NUM_CLASSES)\n",
        "\n",
        "print(f\"üè∑Ô∏è Exemplo de encoding:\")\n",
        "print(f\"   Label original: {y_train[0]}\")\n",
        "print(f\"   One-hot encoded: {y_train_encoded[0]}\")\n",
        "print(f\"\\n‚úÖ Encoding conclu√≠do!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚öñÔ∏è Calcular Class Weights\n",
        "\n",
        "Como o dataset √© muito desbalanceado, vamos calcular **pesos para cada classe**. Classes minorit√°rias ter√£o peso maior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular pesos das classes\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "# Converter para dicion√°rio\n",
        "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "print(\"‚öñÔ∏è Pesos das classes (quanto maior, mais importante):\")\n",
        "for idx, (code, name) in enumerate(class_names.items()):\n",
        "    count = (y_train == idx).sum()\n",
        "    weight = class_weight_dict[idx]\n",
        "    print(f\"   Classe {idx} ({code:6s}): peso = {weight:.2f} ({count:4d} imagens)\")\n",
        "\n",
        "print(\"\\nüí° Classes com menos exemplos ter√£o maior peso durante o treinamento!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 7Ô∏è‚É£ Data Augmentation\n",
        "\n",
        "### üîÑ Por que Data Augmentation?\n",
        "\n",
        "**Data Augmentation** cria varia√ß√µes artificiais das imagens de treino aplicando transforma√ß√µes:\n",
        "- Rota√ß√µes, Zoom, Deslocamentos\n",
        "- Mudan√ßas de brilho, Flips\n",
        "\n",
        "Isso ajuda o modelo a:\n",
        "1. ‚úÖ Generalizar melhor\n",
        "2. ‚úÖ Evitar overfitting\n",
        "3. ‚úÖ Aumentar artificialmente o tamanho do dataset\n",
        "\n",
        "### üé® Configura√ß√£o Agressiva\n",
        "\n",
        "Para imagens de pele, vamos usar transforma√ß√µes agressivas porque les√µes podem aparecer em qualquer orienta√ß√£o."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar gerador de data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=45,           # Rota√ß√£o aleat√≥ria at√© ¬±45 graus\n",
        "    zoom_range=0.2,              # Zoom aleat√≥rio at√© 20%\n",
        "    width_shift_range=0.15,      # Deslocamento horizontal at√© 15%\n",
        "    height_shift_range=0.15,     # Deslocamento vertical at√© 15%\n",
        "    horizontal_flip=True,        # Flip horizontal aleat√≥rio\n",
        "    vertical_flip=True,          # Flip vertical aleat√≥rio\n",
        "    brightness_range=[0.8, 1.2], # Varia√ß√£o de brilho (80% a 120%)\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Ajustar ao conjunto de treino\n",
        "datagen.fit(X_train)\n",
        "\n",
        "print(\"‚úÖ Data Augmentation configurado!\")\n",
        "print(\"\\nüîÑ Transforma√ß√µes aplicadas:\")\n",
        "print(\"   ‚Ä¢ Rota√ß√£o: ¬±45¬∞\")\n",
        "print(\"   ‚Ä¢ Zoom: at√© 20%\")\n",
        "print(\"   ‚Ä¢ Shifts: at√© 15%\")\n",
        "print(\"   ‚Ä¢ Flips: horizontal e vertical\")\n",
        "print(\"   ‚Ä¢ Brilho: 80% a 120%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üëÄ Visualizar Exemplos de Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pegar uma imagem de exemplo\n",
        "sample_img = X_train[0:1]\n",
        "\n",
        "# Gerar 9 vers√µes augmentadas\n",
        "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "# Desnormalizar para visualiza√ß√£o\n",
        "def denormalize(img):\n",
        "    img = img * X_train_std + X_train_mean\n",
        "    return np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "for i, ax in enumerate(axes):\n",
        "    if i == 0:\n",
        "        ax.imshow(denormalize(sample_img[0]))\n",
        "        ax.set_title('Original', fontweight='bold')\n",
        "    else:\n",
        "        augmented = next(datagen.flow(sample_img, batch_size=1))\n",
        "        ax.imshow(denormalize(augmented[0]))\n",
        "        ax.set_title(f'Augmented {i}', fontweight='bold')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Exemplos de Data Augmentation', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Note como as transforma√ß√µes criam varia√ß√µes realistas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 8Ô∏è‚É£ Constru√ß√£o do Modelo CNN\n",
        "\n",
        "## üß† Arquitetura da Rede Neural Convolucional\n",
        "\n",
        "```\n",
        "Input (75, 100, 3)\n",
        "    ‚Üì\n",
        "[ Conv2D(32) x2 + MaxPool + Dropout(25%) ]\n",
        "    ‚Üì\n",
        "[ Conv2D(64) x2 + MaxPool + Dropout(40%) ]\n",
        "    ‚Üì\n",
        "[ Flatten + Dense(128) + Dropout(50%) + Dense(7) ]\n",
        "    ‚Üì\n",
        "Output (7 classes)\n",
        "```\n",
        "\n",
        "### üìö Explica√ß√£o dos Componentes:\n",
        "\n",
        "- **Conv2D**: Aprende filtros para detectar padr√µes (bordas, texturas)\n",
        "- **ReLU**: Fun√ß√£o de ativa√ß√£o n√£o-linear\n",
        "- **MaxPooling**: Reduz dimensionalidade\n",
        "- **Dropout**: Desliga neur√¥nios aleatoriamente para evitar overfitting\n",
        "- **Softmax**: Converte sa√≠da em probabilidades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construir modelo\n",
        "model = Sequential([\n",
        "    # Bloco Convolucional 1\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same', \n",
        "           input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)),\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.25),\n",
        "    \n",
        "    # Bloco Convolucional 2\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.40),\n",
        "    \n",
        "    # Camadas Densas\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(NUM_CLASSES, activation='softmax')\n",
        "], name='SkinCancerCNN')\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìê ARQUITETURA DO MODELO\")\n",
        "print(\"=\"*70)\n",
        "model.summary()\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 9Ô∏è‚É£ Compila√ß√£o do Modelo\n",
        "\n",
        "## üéØ Otimizador Adam\n",
        "\n",
        "**Adam (Adaptive Moment Estimation)** √© um dos melhores otimizadores porque:\n",
        "\n",
        "1. ‚úÖ **Adapta a taxa de aprendizado** para cada par√¢metro\n",
        "2. ‚úÖ **Combina AdaGrad + RMSprop**\n",
        "3. ‚úÖ **Converge rapidamente**\n",
        "4. ‚úÖ **Robusto a hiperpar√¢metros**\n",
        "\n",
        "### üìâ Learning Rate Scheduling (ReduceLROnPlateau)\n",
        "\n",
        "Se a valida√ß√£o parar de melhorar por 3 epochs ‚Üí reduz LR em 50%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar otimizador\n",
        "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "# Compilar modelo\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo compilado!\")\n",
        "print(f\"\\n‚öôÔ∏è Configura√ß√µes:\")\n",
        "print(f\"   Otimizador: Adam\")\n",
        "print(f\"   Learning Rate inicial: {LEARNING_RATE}\")\n",
        "print(f\"   Loss: categorical_crossentropy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîî Configurar Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Callback 1: Reduzir learning rate\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=0.00001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Callback 2: Salvar melhor modelo\n",
        "checkpoint_path = os.path.join(MODELS_DIR, 'best_cnn_model.h5')\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    checkpoint_path,\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Callback 3: Early stopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "callbacks = [reduce_lr, model_checkpoint, early_stop]\n",
        "\n",
        "print(\"‚úÖ Callbacks configurados:\")\n",
        "print(\"   1. ReduceLROnPlateau\")\n",
        "print(\"   2. ModelCheckpoint\")\n",
        "print(\"   3. EarlyStopping\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# üîü Treinamento do Modelo\n",
        "\n",
        "Agora vamos treinar! Isso pode levar **30 min a 1 hora**.\n",
        "\n",
        "**M√©tricas:**\n",
        "- **loss**: erro no treino (‚Üì melhor)\n",
        "- **accuracy**: acur√°cia no treino (‚Üë melhor)\n",
        "- **val_loss**: erro na valida√ß√£o\n",
        "- **val_accuracy**: acur√°cia na valida√ß√£o\n",
        "\n",
        "**Objetivo:** val_accuracy alto e pr√≥ximo de accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üöÄ INICIANDO TREINAMENTO\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Steps por epoch: {len(X_train) // BATCH_SIZE}\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Treinar modelo\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train_encoded, batch_size=BATCH_SIZE),\n",
        "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_val, y_val_encoded),\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weight_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ TREINAMENTO CONCLU√çDO!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ Visualiza√ß√£o do Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotar hist√≥rico\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Accuracy\n",
        "axes[0].plot(history.history['accuracy'], label='Treino', linewidth=2)\n",
        "axes[0].plot(history.history['val_accuracy'], label='Valida√ß√£o', linewidth=2)\n",
        "axes[0].set_title('Acur√°cia', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend(loc='lower right')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss\n",
        "axes[1].plot(history.history['loss'], label='Treino', linewidth=2)\n",
        "axes[1].plot(history.history['val_loss'], label='Valida√ß√£o', linewidth=2)\n",
        "axes[1].set_title('Loss', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend(loc='upper right')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise do treinamento\n",
        "final_train_acc = history.history['accuracy'][-1]\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "best_val_acc = max(history.history['val_accuracy'])\n",
        "\n",
        "print(\"üìä RESUMO DO TREINAMENTO\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Acur√°cia final no treino:     {final_train_acc*100:.2f}%\")\n",
        "print(f\"Acur√°cia final na valida√ß√£o:  {final_val_acc*100:.2f}%\")\n",
        "print(f\"Melhor acur√°cia na valida√ß√£o: {best_val_acc*100:.2f}%\")\n",
        "print(f\"Diferen√ßa treino-valida√ß√£o:   {(final_train_acc - final_val_acc)*100:.2f}%\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if (final_train_acc - final_val_acc) < 0.05:\n",
        "    print(\"\\n‚úÖ Boa generaliza√ß√£o!\")\n",
        "elif (final_train_acc - final_val_acc) < 0.10:\n",
        "    print(\"\\n‚ö†Ô∏è Leve overfitting.\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Overfitting significativo!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 1Ô∏è‚É£2Ô∏è‚É£ Avalia√ß√£o no Conjunto de Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Avaliar no teste\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded, verbose=0)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéØ RESULTADOS NO CONJUNTO DE TESTE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Test Loss:     {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nüìä Compara√ß√£o:\")\n",
        "print(f\"   Valida√ß√£o: {final_val_acc*100:.2f}%\")\n",
        "print(f\"   Teste:     {test_accuracy*100:.2f}%\")\n",
        "\n",
        "if abs(final_val_acc - test_accuracy) < 0.03:\n",
        "    print(\"\\n‚úÖ Resultados consistentes!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è H√° diferen√ßa entre valida√ß√£o e teste.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 1Ô∏è‚É£3Ô∏è‚É£ An√°lise Detalhada: Matriz de Confus√£o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fazer predi√ß√µes no conjunto de valida√ß√£o\n",
        "y_pred_probs = model.predict(X_val, verbose=0)\n",
        "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "y_true_classes = y_val\n",
        "\n",
        "# Calcular matriz de confus√£o\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "# Plotar\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=list(class_names.keys()),\n",
        "            yticklabels=list(class_names.keys()))\n",
        "plt.title('Matriz de Confus√£o', fontsize=16, fontweight='bold')\n",
        "plt.ylabel('Classe Verdadeira')\n",
        "plt.xlabel('Classe Predita')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Diagonal principal = acertos\")\n",
        "print(\"   Fora da diagonal = confus√µes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 1Ô∏è‚É£4Ô∏è‚É£ M√©tricas Por Classe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Relat√≥rio de classifica√ß√£o\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä RELAT√ìRIO DE CLASSIFICA√á√ÉO\")\n",
        "print(\"=\"*80)\n",
        "report = classification_report(\n",
        "    y_true_classes, \n",
        "    y_pred_classes, \n",
        "    target_names=list(class_names.values()),\n",
        "    digits=4\n",
        ")\n",
        "print(report)\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüìö Gloss√°rio:\")\n",
        "print(\"   ‚Ä¢ Precision: Acertos / Total predito como classe X\")\n",
        "print(\"   ‚Ä¢ Recall: Acertos / Total real da classe X\")\n",
        "print(\"   ‚Ä¢ F1-Score: M√©dia harm√¥nica de Precision e Recall\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Taxa de Erro Por Classe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular taxa de erro por classe\n",
        "class_correct = np.diag(cm)\n",
        "class_total = np.sum(cm, axis=1)\n",
        "class_accuracy = class_correct / class_total\n",
        "class_error = 1 - class_accuracy\n",
        "\n",
        "# Plotar\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "x_pos = np.arange(len(class_names))\n",
        "\n",
        "bars = ax.bar(x_pos, class_error, edgecolor='black')\n",
        "ax.set_xlabel('Classe')\n",
        "ax.set_ylabel('Taxa de Erro')\n",
        "ax.set_title('Taxa de Erro por Classe', fontsize=16, fontweight='bold')\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(list(class_names.keys()))\n",
        "ax.set_ylim([0, 1])\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Valores nas barras\n",
        "for bar, error in zip(bars, class_error):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "            f'{error*100:.1f}%', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# An√°lise\n",
        "worst_idx = np.argmax(class_error)\n",
        "best_idx = np.argmin(class_error)\n",
        "worst_class = list(class_names.keys())[worst_idx]\n",
        "best_class = list(class_names.keys())[best_idx]\n",
        "\n",
        "print(f\"\\nüìà Classe com MAIOR erro: {worst_class} ({class_error[worst_idx]*100:.2f}%)\")\n",
        "print(f\"üìâ Classe com MENOR erro: {best_class} ({class_error[best_idx]*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 1Ô∏è‚É£5Ô∏è‚É£ Salvar Modelo Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Salvar modelo final\n",
        "final_model_path = os.path.join(MODELS_DIR, 'cnn_skin_cancer_final.h5')\n",
        "model.save(final_model_path)\n",
        "\n",
        "print(\"üíæ Modelos salvos:\")\n",
        "print(f\"   1. Melhor: {checkpoint_path}\")\n",
        "print(f\"   2. Final:  {final_model_path}\")\n",
        "print(\"\\n‚úÖ Carregar depois com:\")\n",
        "print(\"   model = keras.models.load_model('caminho.h5')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 1Ô∏è‚É£6Ô∏è‚É£ Conclus√µes e Pr√≥ximos Passos\n",
        "\n",
        "## üìù Resumo\n",
        "\n",
        "Implementamos uma **CNN do zero** para classificar les√µes de pele usando apenas imagens.\n",
        "\n",
        "### ‚úÖ O que fizemos:\n",
        "\n",
        "1. Carregamos e exploramos HAM10000 (10k+ imagens, 7 classes)\n",
        "2. Processamos imagens (resize, normaliza√ß√£o)\n",
        "3. Lidamos com desbalanceamento (class weights)\n",
        "4. Aplicamos Data Augmentation agressivo\n",
        "5. Constru√≠mos CNN com regulariza√ß√£o\n",
        "6. Treinamos com Adam + learning rate scheduling\n",
        "7. Avaliamos com m√©tricas detalhadas\n",
        "\n",
        "### üéØ Resultados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üèÜ RESULTADOS FINAIS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Acur√°cia no Treino:      {final_train_acc*100:.2f}%\")\n",
        "print(f\"Acur√°cia na Valida√ß√£o:   {final_val_acc*100:.2f}%\")\n",
        "print(f\"Acur√°cia no Teste:       {test_accuracy*100:.2f}%\")\n",
        "print(f\"\\nMelhor Val Accuracy:     {best_val_acc*100:.2f}%\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîç Observa√ß√µes:\n",
        "\n",
        "1. **Dataset Desbalanceado**: Usamos class weights\n",
        "2. **Data Augmentation √© Crucial**: Evita overfitting\n",
        "3. **Regulariza√ß√£o**: Dropout + LR scheduling ajudam\n",
        "4. **Limita√ß√µes**: Imagens pequenas (100x75) perdem detalhes\n",
        "\n",
        "### üöÄ Pr√≥ximos Passos:\n",
        "\n",
        "1. **Vision Transformer (ViT)**: Implementar e comparar\n",
        "2. **Compara√ß√£o com VLMs**: Testar Gemini, GPT-4V, Claude\n",
        "3. **Melhorias**:\n",
        "   - Transfer Learning (ResNet50, EfficientNet)\n",
        "   - Imagens maiores (224x224)\n",
        "   - Ensemble de modelos\n",
        "\n",
        "### üí° Li√ß√µes:\n",
        "\n",
        "- ‚úÖ CNNs funcionam bem para imagens m√©dicas\n",
        "- ‚úÖ Data Augmentation e regulariza√ß√£o s√£o essenciais\n",
        "- ‚úÖ Desbalanceamento requer aten√ß√£o especial\n",
        "- ‚úÖ An√°lise detalhada revela insights importantes\n",
        "\n",
        "---\n",
        "\n",
        "## üéì Fim do Notebook\n",
        "\n",
        "**Modelo baseline criado!** Pronto para comparar com arquiteturas avan√ßadas.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
